{"type":"getPostByPath","data":{"title":"[tutorial] Enhancing Commit Messages with commitollama: A Guide for VSCode and Local LLM Integration","date":"2024-09-03T16:00:00.000Z","description":"<h1>üìå Introduction</h1>\n<p>This article introduces commitollama, an alternative to GitHub Copilot designed for generating commit messages using local LLMs, ensuring privacy for confidential projects. It outlines the installation process for the commitollama extension in VSCode and necessary setup steps to start using it effectively.</p>","categories":[],"tags":[{"name":"ollama","_id":"cmhs5lvoe005amp3x1jay5yjo"}],"content":"\n    <style>\n    .post-lang-switch{\n      position: absolute;\n      top: .75rem;\n      right: .75rem;\n      z-index: 2;\n      color: var(--link, #3273dc);\n    }\n    .post-lang-switch:hover{opacity:.8}\n    </style>\n  \n    <a class=\"post-lang-switch\" href=\"/blog/zh-TW/tutorial-enhancing-commit-messages-with-commitollama-a-guide-for-vscode-and-local-llm-integration/\" title=\"Êü•ÁúãÁπÅÈ´î‰∏≠ÊñáÁâàÊú¨\" aria-label=\"Êü•ÁúãÁπÅÈ´î‰∏≠ÊñáÁâàÊú¨\">\n      <i class=\"fas fa-language\"></i>\n    </a>\n  <h1>üìå Introduction</h1>\n<p>This article introduces commitollama, an alternative to GitHub Copilot designed for generating commit messages using local LLMs, ensuring privacy for confidential projects. It outlines the installation process for the commitollama extension in VSCode and necessary setup steps to start using it effectively.</p>\n<span id=\"more\"></span>\n<h1>üöÄ Quick Start</h1>\n<p><img src=\"https://commitollama.gallerycdn.vsassets.io/extensions/commitollama/commitollama/1.7.2/1723710671949/Microsoft.VisualStudio.Services.Icons.Default\" alt=\"\"></p>\n<h2 id=\"How-to-use\">How to use</h2>\n<ol>\n<li>Install the extension in VSCode.</li>\n<li>Install Ollama to integrate the LLM.</li>\n</ol>\n<p><img src=\"https://hackmd.io/_uploads/r1Vdxl8nR.png\" alt=\"Screenshot from 2024-09-04 22-35-57\"></p>\n<p><img src=\"https://hackmd.io/_uploads/Bk-6gx830.png\" alt=\"Screenshot from 2024-09-04 22-37-24\"></p>\n<h3 id=\"Installing-Ollama\">Installing Ollama</h3>\n<p>Run the following command to install Ollama:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -fsSL https://ollama.com/install.sh | sh</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://hackmd.io/_uploads/rJwuUxIn0.png\" alt=\"Screenshot from 2024-09-04 23-01-51\"></p>\n<p>After installation, you can run Ollama using:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama</span><br></pre></td></tr></table></figure>\n<p>This will display a list of available commands:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Usage:</span><br><span class=\"line\">  ollama [flags]</span><br><span class=\"line\">  ollama [command]</span><br><span class=\"line\"></span><br><span class=\"line\">Available Commands:</span><br><span class=\"line\">  serve       Start ollama</span><br><span class=\"line\">  create      Create a model <span class=\"keyword\">from</span> a Modelfile</span><br><span class=\"line\">  show        Show information <span class=\"keyword\">for</span> a model</span><br><span class=\"line\">  run         Run a model</span><br><span class=\"line\">  pull        Pull a model <span class=\"keyword\">from</span> a registry</span><br><span class=\"line\">  push        Push a model to a registry</span><br><span class=\"line\">  <span class=\"built_in\">list</span>        <span class=\"type\">List</span> models</span><br><span class=\"line\">  ps          <span class=\"type\">List</span> running models</span><br><span class=\"line\">  cp          Copy a model</span><br><span class=\"line\">  rm          Remove a model</span><br><span class=\"line\">  <span class=\"built_in\">help</span>        Help about <span class=\"built_in\">any</span> command</span><br><span class=\"line\"></span><br><span class=\"line\">Flags:</span><br><span class=\"line\">  -h, --<span class=\"built_in\">help</span>      <span class=\"built_in\">help</span> <span class=\"keyword\">for</span> ollama</span><br><span class=\"line\">  -v, --version   Show version information</span><br><span class=\"line\"></span><br><span class=\"line\">Use <span class=\"string\">&quot;ollama [command] --help&quot;</span> <span class=\"keyword\">for</span> more information about a command.</span><br></pre></td></tr></table></figure>\n<!-- In this case, we will use `tavernari/git-commit-message` as our LLM model. This model is trained on Mistral0.3 . -->\n<p>Download the Phi3 model (3.8b) by running:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama pull phi3:3.8b</span><br></pre></td></tr></table></figure>\n<p>Start the Ollama service using:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama serve</span><br></pre></td></tr></table></figure>\n<!-- If there a error message `Error: listen tcp 127.0.0.1:11434: bind: address already in use`. Which can be found at [there](https://github.com/ollama/ollama/issues/707) . You need to shutdown the ollama and restart it. -->\n<p>If you encounter the error message <code>Error: listen tcp 127.0.0.1:11434: bind: address already in use</code>, you can find a solution <a href=\"https://github.com/ollama/ollama/issues/707\">here</a> .</p>\n<p>To restart Ollama, stop the current service and relaunch it:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl stop ollama.service</span><br><span class=\"line\">ollama serve</span><br></pre></td></tr></table></figure>\n<p>To prevent the model from being deleted after downloading, refer to this discussion <a href=\"https://github.com/ollama/ollama/issues/1493\">here</a> .</p>\n<h3 id=\"Setting-Up-VSCode\">Setting Up VSCode</h3>\n<ul>\n<li>After installing the extension, use a custom model for commit message generation.</li>\n<li>Press the button in the interface to automatically generate the commit message.</li>\n</ul>\n<p><img src=\"https://hackmd.io/_uploads/HklK2W82C.png\" alt=\"image\"></p>\n<h1>üîÅ Recap</h1>\n<ul>\n<li>commitollama is a privacy-focused commit message generator alternative to GitHub Copilot.</li>\n<li>The tool leverages open-source LLMs like Llama, Mistral, and Phi3.</li>\n<li>Easy integration with VSCode through a simple extension installation process.</li>\n<li>Users can easily retrieve models, run services, and generate commit messages efficiently.</li>\n</ul>\n<h1>üîó References</h1>\n<ul>\n<li><a href=\"https://github.com/ollama/ollama/issues/707\">https://github.com/ollama/ollama/issues/707</a></li>\n<li><a href=\"https://github.com/ollama/ollama/issues/1493\">https://github.com/ollama/ollama/issues/1493</a></li>\n</ul>\n","_path":"/tutorial-enhancing-commit-messages-with-commitollama-a-guide-for-vscode-and-local-llm-integration.en/","_link":"https://hsiangjenli.github.io/blog/tutorial-enhancing-commit-messages-with-commitollama-a-guide-for-vscode-and-local-llm-integration.en/","_id":"cmhs5lvo2003emp3xbgnsaxe2"}}