{"type":"getPostById","data":{"title":"[tutorial] Enhancing Commit Messages with commitollama: A Guide for VSCode and Local LLM Integration","date":"2024-09-04T00:00:00.000Z","description":"<h2 id=\"commitollama\"><a href=\"#commitollama\" class=\"headerlink\" title=\"commitollama\"></a>commitollama</h2>","categories":[],"tags":[{"name":"ollama","_id":"cm6etvtzo001rhoonae5d1iml"}],"content":"<h2 id=\"commitollama\"><a href=\"#commitollama\" class=\"headerlink\" title=\"commitollama\"></a>commitollama</h2><span id=\"more\"></span>\n\n<p><img src=\"https://commitollama.gallerycdn.vsassets.io/extensions/commitollama/commitollama/1.7.2/1723710671949/Microsoft.VisualStudio.Services.Icons.Default\"></p>\n<!-- The commitollama is an alternative of github copilot commit generator which based on open source llama (llama3, gemma, mistral, phi3 etc). If your project is confidential, you can use local LLM to ensure privacy issue. -->\n\n<p><strong>commitollama</strong> is an alternative to GitHub Copilotâ€™s commit message generator, powered by open-source models such as Llama (Llama3, Gemma, Mistral, Phi3, etc. For projects where confidentiality is a concern, commitollama allows you to use a local Large Language Model (LLM), ensuring privacy.</p>\n<h2 id=\"How-to-use\"><a href=\"#How-to-use\" class=\"headerlink\" title=\"How to use\"></a>How to use</h2><!-- Thanks to the contributors, commitollama can be directly used by simply install the extension in vscode and ollama -->\n\n<p>Thanks to its contributors, commitollama can be directly integrated into VSCode by installing the extension and setting up Ollama.</p>\n<ol>\n<li>Install the extension in VSCode.</li>\n<li>Install Ollama to integrate the LLM.</li>\n</ol>\n<p><img src=\"https://hackmd.io/_uploads/r1Vdxl8nR.png\" alt=\"Screenshot from 2024-09-04 22-35-57\"></p>\n<p><img src=\"https://hackmd.io/_uploads/Bk-6gx830.png\" alt=\"Screenshot from 2024-09-04 22-37-24\"></p>\n<h3 id=\"Installing-Ollama\"><a href=\"#Installing-Ollama\" class=\"headerlink\" title=\"Installing Ollama\"></a>Installing Ollama</h3><p>Run the following command to install Ollama:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -fsSL https://ollama.com/install.sh | sh</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://hackmd.io/_uploads/rJwuUxIn0.png\" alt=\"Screenshot from 2024-09-04 23-01-51\"></p>\n<p>After installation, you can run Ollama using:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama</span><br></pre></td></tr></table></figure>\n<p>This will display a list of available commands:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Usage:</span><br><span class=\"line\">  ollama [flags]</span><br><span class=\"line\">  ollama [command]</span><br><span class=\"line\"></span><br><span class=\"line\">Available Commands:</span><br><span class=\"line\">  serve       Start ollama</span><br><span class=\"line\">  create      Create a model <span class=\"keyword\">from</span> a Modelfile</span><br><span class=\"line\">  show        Show information <span class=\"keyword\">for</span> a model</span><br><span class=\"line\">  run         Run a model</span><br><span class=\"line\">  pull        Pull a model <span class=\"keyword\">from</span> a registry</span><br><span class=\"line\">  push        Push a model to a registry</span><br><span class=\"line\">  <span class=\"built_in\">list</span>        <span class=\"type\">List</span> models</span><br><span class=\"line\">  ps          <span class=\"type\">List</span> running models</span><br><span class=\"line\">  cp          Copy a model</span><br><span class=\"line\">  rm          Remove a model</span><br><span class=\"line\">  <span class=\"built_in\">help</span>        Help about <span class=\"built_in\">any</span> command</span><br><span class=\"line\"></span><br><span class=\"line\">Flags:</span><br><span class=\"line\">  -h, --<span class=\"built_in\">help</span>      <span class=\"built_in\">help</span> <span class=\"keyword\">for</span> ollama</span><br><span class=\"line\">  -v, --version   Show version information</span><br><span class=\"line\"></span><br><span class=\"line\">Use <span class=\"string\">&quot;ollama [command] --help&quot;</span> <span class=\"keyword\">for</span> more information about a command.</span><br></pre></td></tr></table></figure>\n\n<!-- In this case, we will use `tavernari/git-commit-message` as our LLM model. This model is trained on Mistral0.3 . -->\n\n<p>Download the Phi3 model (3.8b) by running:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama pull phi3:3.8b</span><br></pre></td></tr></table></figure>\n\n<p>Start the Ollama service using:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama serve</span><br></pre></td></tr></table></figure>\n\n<!-- If there a error message `Error: listen tcp 127.0.0.1:11434: bind: address already in use`. Which can be found at [there](https://github.com/ollama/ollama/issues/707) . You need to shutdown the ollama and restart it. -->\n\n\n<p>If you encounter the error message <code>Error: listen tcp 127.0.0.1:11434: bind: address already in use</code>, you can find a solution <a href=\"https://github.com/ollama/ollama/issues/707\">here</a> .</p>\n<p>To restart Ollama, stop the current service and relaunch it:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl stop ollama.service</span><br><span class=\"line\">ollama serve</span><br></pre></td></tr></table></figure>\n\n<p>To prevent the model from being deleted after downloading, refer to this discussion <a href=\"https://github.com/ollama/ollama/issues/1493\">here</a> .</p>\n<h3 id=\"Setting-Up-VSCode\"><a href=\"#Setting-Up-VSCode\" class=\"headerlink\" title=\"Setting Up VSCode\"></a>Setting Up VSCode</h3><ul>\n<li>After installing the extension, use a custom model for commit message generation.</li>\n<li>Press the button in the interface to automatically generate the commit message.</li>\n</ul>\n<p><img src=\"https://hackmd.io/_uploads/HklK2W82C.png\" alt=\"image\"></p>\n","_path":"tutorial_commitollama/","_link":"https://hsiangjenli.github.io/blog/tutorial_commitollama/","_id":"cm6etvtze000shoon68in3umz"}}