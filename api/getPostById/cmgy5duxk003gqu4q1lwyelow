{"type":"getPostById","data":{"title":"[tutorial] Using Ollama with OpenCommit for Local Commit Message Generation","date":"2024-08-28T16:00:00.000Z","description":"<h1>📌 Introduction</h1>\n<p>This article covers using Ollama with OpenCommit for generating commit messages locally. It includes an overview of running Ollama in a Docker container, instructions for using the Ollama CLI, and how to combine Ollama with OpenCommit for generating commit messages.</p>","categories":[],"tags":[{"name":"ollama","_id":"cmgy5duxs004uqu4q7itv0jqw"}],"content":"<h1>📌 Introduction</h1>\n<p>This article covers using Ollama with OpenCommit for generating commit messages locally. It includes an overview of running Ollama in a Docker container, instructions for using the Ollama CLI, and how to combine Ollama with OpenCommit for generating commit messages.</p>\n<span id=\"more\"></span>\n<h1>🚀 Quick Start</h1>\n<h3 id=\"Start-a-container\">Start a container</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama-commit ollama/ollama:0.3.6</span><br></pre></td></tr></table></figure>\n<h3 id=\"Enter-the-Docker-container\">Enter the Docker container</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker exec -it ollama-commit bash</span><br></pre></td></tr></table></figure>\n<h3 id=\"Pull-model\">Pull model</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama run gemma2:2b</span><br></pre></td></tr></table></figure>\n<h3 id=\"Start-a-chat\">Start a chat</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; Send a message (/? <span class=\"keyword\">for</span> <span class=\"built_in\">help</span>)</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"Exit-the-chat\">Exit the chat</h3>\n<p>Type <code>/bye</code> to exit the chat.</p>\n<h3 id=\"Install-opencommit\">Install opencommit</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install -g opencommit</span><br></pre></td></tr></table></figure>\n<h3 id=\"Generate-commit-messages-with-local-ollama-server\">Generate commit messages with local ollama server</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">OCO_AI_PROVIDER=&#x27;ollama/gemma2:2b&#x27; opencommit</span><br></pre></td></tr></table></figure>\n<p>output:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">┌  open-commit</span><br><span class=\"line\">│</span><br><span class=\"line\">◇  1 staged files:</span><br><span class=\"line\">  README.md</span><br><span class=\"line\">│</span><br><span class=\"line\">◇  📝 Commit message generated</span><br><span class=\"line\">│</span><br><span class=\"line\">└  Generated commit message:</span><br><span class=\"line\">——————————————————</span><br><span class=\"line\">feat(README.md): add link to Ollama website and examples of running models</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">——————————————————</span><br><span class=\"line\"></span><br><span class=\"line\">│</span><br><span class=\"line\">◇  Confirm the commit message?</span><br><span class=\"line\">│  No</span><br><span class=\"line\">│</span><br><span class=\"line\">◇  Do you want to regenerate the message ?</span><br><span class=\"line\">│  Yes</span><br><span class=\"line\">│</span><br><span class=\"line\">◇  📝 Commit message generated</span><br><span class=\"line\">│</span><br><span class=\"line\">└  Generated commit message:</span><br><span class=\"line\">——————————————————</span><br><span class=\"line\">feat(README.md): add link to Ollama website and examples of model usage in README.md </span><br></pre></td></tr></table></figure>\n<h2 id=\"Error-code-127\">Error code 127</h2>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Error: llama runner process has terminated: exit status 127</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; [GIN] 2024/08/28 - 18:43:24 | 200 |      68.455µs |       127.0.0.1 | HEAD     <span class=\"string\">&quot;/&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; [GIN] 2024/08/28 - 18:43:24 | 200 |    7.845273ms |       127.0.0.1 | POST     <span class=\"string\">&quot;/api/show&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.392Z level=INFO <span class=\"built_in\">source</span>=memory.go:309 msg=<span class=\"string\">&quot;offload to cpu&quot;</span> layers.requested=-1 layers.model=33 layers.offload=0 layers.split=<span class=\"string\">&quot;&quot;</span> memory.available=<span class=\"string\">&quot;[7.3 GiB]&quot;</span> memory.required.full=<span class=\"string\">&quot;5.5 GiB&quot;</span> memory.required.partial=<span class=\"string\">&quot;0 B&quot;</span> memory.required.kv=<span class=\"string\">&quot;1.0 GiB&quot;</span> memory.required.allocations=<span class=\"string\">&quot;[5.5 GiB]&quot;</span> memory.weights.total=<span class=\"string\">&quot;4.7 GiB&quot;</span> memory.weights.repeating=<span class=\"string\">&quot;4.6 GiB&quot;</span> memory.weights.nonrepeating=<span class=\"string\">&quot;105.0 MiB&quot;</span> memory.graph.full=<span class=\"string\">&quot;560.0 MiB&quot;</span> memory.graph.partial=<span class=\"string\">&quot;585.0 MiB&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.392Z level=INFO <span class=\"built_in\">source</span>=server.go:391 msg=<span class=\"string\">&quot;starting llama server&quot;</span> cmd=<span class=\"string\">&quot;/tmp/ollama731275887/runners/cpu/ollama_llama_server --model /root/.ollama/models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 --ctx-size 8192 --batch-size 512 --embedding --log-disable --no-mmap --parallel 4 --port 33357&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.392Z level=INFO <span class=\"built_in\">source</span>=sched.go:450 msg=<span class=\"string\">&quot;loaded runners&quot;</span> count=1</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.393Z level=INFO <span class=\"built_in\">source</span>=server.go:591 msg=<span class=\"string\">&quot;waiting for llama runner to start responding&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.393Z level=INFO <span class=\"built_in\">source</span>=server.go:625 msg=<span class=\"string\">&quot;waiting for server to become available&quot;</span> status=<span class=\"string\">&quot;llm server error&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; /tmp/ollama731275887/runners/cpu/ollama_llama_server: error <span class=\"keyword\">while</span> loading shared libraries: libllama.so: cannot open shared object file: No such file or directory</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.644Z level=ERROR <span class=\"built_in\">source</span>=sched.go:456 msg=<span class=\"string\">&quot;error loading llama server&quot;</span> error=<span class=\"string\">&quot;llama runner process has terminated: exit status 127&quot;</span></span></span><br><span class=\"line\">[GIN] 2024/08/28 - 18:43:24 | 500 |  266.021797ms |       127.0.0.1 | POST     &quot;/api/chat&quot;</span><br></pre></td></tr></table></figure>\n<p>The error code occurs when the Docker image version is greater than <code>0.3.6</code>. Therefore, you need to pull the ollama image with version <code>0.3.6</code> and run the container. Click <a href=\"https://github.com/ollama/ollama/issues/6541\">here</a> to view the discussion.</p>\n<h1>🔁 Recap</h1>\n<ul>\n<li>Ollama allows for the generation of commit messages using AI models.</li>\n<li>The article details setting up Ollama in a Docker environment.</li>\n<li>OpenCommit is integrated to simplify the process of generating commit messages using an AI model.</li>\n<li>Users can interact with the AI model through a chat interface.</li>\n</ul>\n<h1>🔗 References</h1>\n<ul>\n<li><a href=\"https://ollama.com/models\">https://ollama.com/models</a></li>\n<li><a href=\"https://github.com/ollama/ollama/issues/6541\">https://github.com/ollama/ollama/issues/6541</a></li>\n</ul>\n","_path":"tutorial_ollama_opencommit/","_link":"https://hsiangjenli.github.io/blog/tutorial_ollama_opencommit/","_id":"cmgy5duxk003gqu4q1lwyelow"}}