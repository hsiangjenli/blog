{"type":"getPostById","data":{"title":"[tutorial] 使用 commitollama 強化提交訊息：VSCode 與本地 LLM 整合指南","date":"2024-09-03T16:00:00.000Z","description":"<blockquote>\n<p>註記：此頁為由 AI（gpt-5-mini-2025-08-07）自動翻譯自英文原文，可能含有少量不準確之處。</p>\n</blockquote>\n<h1>📌 介紹</h1>\n<p>本文介紹 commitollama，這是一個用於產生提交訊息（commit messages）的工具，作為 GitHub Copilot 的替代方案，採用本地 LLM 以確保機密專案的隱私。本文說明在 VSCode 中安裝 commitollama 擴充套件的流程以及開始使用所需的設定步驟。</p>","categories":[],"tags":[{"name":"ollama","_id":"cmflkey26004cms2b88fl0oq8"}],"content":"<blockquote>\n<p>註記：此頁為由 AI（gpt-5-mini-2025-08-07）自動翻譯自英文原文，可能含有少量不準確之處。</p>\n</blockquote>\n<h1>📌 介紹</h1>\n<p>本文介紹 commitollama，這是一個用於產生提交訊息（commit messages）的工具，作為 GitHub Copilot 的替代方案，採用本地 LLM 以確保機密專案的隱私。本文說明在 VSCode 中安裝 commitollama 擴充套件的流程以及開始使用所需的設定步驟。</p>\n<span id=\"more\"></span>\n<h1>🚀 快速開始</h1>\n<p><img src=\"https://commitollama.gallerycdn.vsassets.io/extensions/commitollama/commitollama/1.7.2/1723710671949/Microsoft.VisualStudio.Services.Icons.Default\" alt=\"\"></p>\n<h2 id=\"使用方式\">使用方式</h2>\n<ol>\n<li>在 VSCode 中安裝該擴充套件。</li>\n<li>安裝 Ollama 以整合 LLM。</li>\n</ol>\n<p><img src=\"https://hackmd.io/_uploads/r1Vdxl8nR.png\" alt=\"截圖：2024-09-04 22-35-57\"></p>\n<p><img src=\"https://hackmd.io/_uploads/Bk-6gx830.png\" alt=\"截圖：2024-09-04 22-37-24\"></p>\n<h3 id=\"安裝-Ollama\">安裝 Ollama</h3>\n<p>執行以下指令以安裝 Ollama：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -fsSL https://ollama.com/install.sh | sh</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://hackmd.io/_uploads/rJwuUxIn0.png\" alt=\"截圖：2024-09-04 23-01-51\"></p>\n<p>安裝完成後，可透過下列方式執行 Ollama：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama</span><br></pre></td></tr></table></figure>\n<p>這會顯示可用指令清單：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Usage:</span><br><span class=\"line\">  ollama [flags]</span><br><span class=\"line\">  ollama [command]</span><br><span class=\"line\"></span><br><span class=\"line\">Available Commands:</span><br><span class=\"line\">  serve       Start ollama</span><br><span class=\"line\">  create      Create a model <span class=\"keyword\">from</span> a Modelfile</span><br><span class=\"line\">  show        Show information <span class=\"keyword\">for</span> a model</span><br><span class=\"line\">  run         Run a model</span><br><span class=\"line\">  pull        Pull a model <span class=\"keyword\">from</span> a registry</span><br><span class=\"line\">  push        Push a model to a registry</span><br><span class=\"line\">  <span class=\"built_in\">list</span>        <span class=\"type\">List</span> models</span><br><span class=\"line\">  ps          <span class=\"type\">List</span> running models</span><br><span class=\"line\">  cp          Copy a model</span><br><span class=\"line\">  rm          Remove a model</span><br><span class=\"line\">  <span class=\"built_in\">help</span>        Help about <span class=\"built_in\">any</span> command</span><br><span class=\"line\"></span><br><span class=\"line\">Flags:</span><br><span class=\"line\">  -h, --<span class=\"built_in\">help</span>      <span class=\"built_in\">help</span> <span class=\"keyword\">for</span> ollama</span><br><span class=\"line\">  -v, --version   Show version information</span><br><span class=\"line\"></span><br><span class=\"line\">Use <span class=\"string\">&quot;ollama [command] --help&quot;</span> <span class=\"keyword\">for</span> more information about a command.</span><br></pre></td></tr></table></figure>\n<!-- In this case, we will use `tavernari/git-commit-message` as our LLM model. This model is trained on Mistral0.3 . -->\n<p>下載 Phi3 模型 (3.8b)，執行：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama pull phi3:3.8b</span><br></pre></td></tr></table></figure>\n<p>啟動 Ollama 服務：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama serve</span><br></pre></td></tr></table></figure>\n<!-- If there a error message `Error: listen tcp 127.0.0.1:11434: bind: address already in use`. Which can be found at [there](https://github.com/ollama/ollama/issues/707) . You need to shutdown the ollama and restart it. -->\n<p>如果遇到錯誤訊息 <code>Error: listen tcp 127.0.0.1:11434: bind: address already in use</code>，可在 <a href=\"https://github.com/ollama/ollama/issues/707\">這裡</a> 找到解法。</p>\n<p>要重新啟動 Ollama，先停止目前服務再重新啟動：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl stop ollama.service</span><br><span class=\"line\">ollama serve</span><br></pre></td></tr></table></figure>\n<p>若要避免下載後模型被刪除，請參閱此討論 <a href=\"https://github.com/ollama/ollama/issues/1493\">這裡</a> 。</p>\n<h3 id=\"在-VSCode-中設定\">在 VSCode 中設定</h3>\n<ul>\n<li>安裝擴充套件後，可使用自訂模型來產生提交訊息。</li>\n<li>在介面中按下按鈕即可自動產生提交訊息。</li>\n</ul>\n<p><img src=\"https://hackmd.io/_uploads/HklK2W82C.png\" alt=\"圖片\"></p>\n<h1>🔁 回顧</h1>\n<ul>\n<li>commitollama 是一個重視隱私的提交訊息產生器，作為 GitHub Copilot 的替代方案。</li>\n<li>該工具利用開放原始碼 LLM，例如 Llama、Mistral 與 Phi3。</li>\n<li>透過簡單的擴充套件安裝程序即可與 VSCode 輕鬆整合。</li>\n<li>使用者可以輕鬆取得模型、啟動服務並有效率地生成提交訊息。</li>\n</ul>\n<h1>🔗 參考資料</h1>\n<ul>\n<li><a href=\"https://github.com/ollama/ollama/issues/707\">https://github.com/ollama/ollama/issues/707</a></li>\n<li><a href=\"https://github.com/ollama/ollama/issues/1493\">https://github.com/ollama/ollama/issues/1493</a></li>\n</ul>\n","_path":"tutorial-enhancing-commit-messages-with-commitollama-a-guide-for-vscode-and-local-llm-integration.zh-TW/","_link":"https://hsiangjenli.github.io/blog/tutorial-enhancing-commit-messages-with-commitollama-a-guide-for-vscode-and-local-llm-integration.zh-TW/","_id":"cmflkey1t002dms2b0c65a92w"}}