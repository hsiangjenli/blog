{"type":"getPostById","data":{"title":"[tutorial] 使用 commitollama 改善提交訊息：VSCode 與本地 LLM 整合指南","date":"2024-09-03T16:00:00.000Z","description":"<blockquote>\n<p>註記：此頁為由 AI（gpt-5-mini-2025-08-07）自動翻譯自英文原文，可能含有少量不準確之處。</p>\n<h1>📌 介紹</h1>\n</blockquote>\n<p>本篇文章介紹 commitollama，一個用於生成提交訊息且以本地 LLM 為基礎的替代方案，適合需保護專案隱私的情境。本文說明在 VSCode 中安裝 commitollama 延伸功能的流程以及開始使用前的必要設定。</p>","categories":[],"tags":[{"name":"ollama","_id":"cmfj10y56004cn42b708qbr4t"}],"content":"<blockquote>\n<p>註記：此頁為由 AI（gpt-5-mini-2025-08-07）自動翻譯自英文原文，可能含有少量不準確之處。</p>\n<h1>📌 介紹</h1>\n</blockquote>\n<p>本篇文章介紹 commitollama，一個用於生成提交訊息且以本地 LLM 為基礎的替代方案，適合需保護專案隱私的情境。本文說明在 VSCode 中安裝 commitollama 延伸功能的流程以及開始使用前的必要設定。</p>\n<span id=\"more\"></span>\n<h1>🚀 快速開始</h1>\n<p><img src=\"https://commitollama.gallerycdn.vsassets.io/extensions/commitollama/commitollama/1.7.2/1723710671949/Microsoft.VisualStudio.Services.Icons.Default\" alt=\"\"></p>\n<h2 id=\"使用方式\">使用方式</h2>\n<ol>\n<li>在 VSCode 中安裝此延伸功能。</li>\n<li>安裝 Ollama，以整合 LLM。</li>\n</ol>\n<p><img src=\"https://hackmd.io/_uploads/r1Vdxl8nR.png\" alt=\"Screenshot from 2024-09-04 22-35-57\"></p>\n<p><img src=\"https://hackmd.io/_uploads/Bk-6gx830.png\" alt=\"Screenshot from 2024-09-04 22-37-24\"></p>\n<h3 id=\"安裝-Ollama\">安裝 Ollama</h3>\n<p>執行以下指令以安裝 Ollama：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -fsSL https://ollama.com/install.sh | sh</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://hackmd.io/_uploads/rJwuUxIn0.png\" alt=\"Screenshot from 2024-09-04 23-01-51\"></p>\n<p>安裝完成後，可執行 Ollama：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama</span><br></pre></td></tr></table></figure>\n<p>這會顯示可用的指令列表：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Usage:</span><br><span class=\"line\">  ollama [flags]</span><br><span class=\"line\">  ollama [command]</span><br><span class=\"line\"></span><br><span class=\"line\">Available Commands:</span><br><span class=\"line\">  serve       Start ollama</span><br><span class=\"line\">  create      Create a model <span class=\"keyword\">from</span> a Modelfile</span><br><span class=\"line\">  show        Show information <span class=\"keyword\">for</span> a model</span><br><span class=\"line\">  run         Run a model</span><br><span class=\"line\">  pull        Pull a model <span class=\"keyword\">from</span> a registry</span><br><span class=\"line\">  push        Push a model to a registry</span><br><span class=\"line\">  <span class=\"built_in\">list</span>        <span class=\"type\">List</span> models</span><br><span class=\"line\">  ps          <span class=\"type\">List</span> running models</span><br><span class=\"line\">  cp          Copy a model</span><br><span class=\"line\">  rm          Remove a model</span><br><span class=\"line\">  <span class=\"built_in\">help</span>        Help about <span class=\"built_in\">any</span> command</span><br><span class=\"line\"></span><br><span class=\"line\">Flags:</span><br><span class=\"line\">  -h, --<span class=\"built_in\">help</span>      <span class=\"built_in\">help</span> <span class=\"keyword\">for</span> ollama</span><br><span class=\"line\">  -v, --version   Show version information</span><br><span class=\"line\"></span><br><span class=\"line\">Use <span class=\"string\">&quot;ollama [command] --help&quot;</span> <span class=\"keyword\">for</span> more information about a command.</span><br></pre></td></tr></table></figure>\n<!-- 在此範例中，我們將使用 `tavernari/git-commit-message` 作為 LLM 模型。該模型以 Mistral0.3 訓練。 -->\n<p>下載 Phi3 模型（3.8b），執行：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama pull phi3:3.8b</span><br></pre></td></tr></table></figure>\n<p>啟動 Ollama 服務：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama serve</span><br></pre></td></tr></table></figure>\n<!-- 若遇到錯誤訊息 `Error: listen tcp 127.0.0.1:11434: bind: address already in use`，可參考此處的討論：https://github.com/ollama/ollama/issues/707。你需要關閉 ollama 並重新啟動。 -->\n<p>如果你遇到錯誤訊息 <code>Error: listen tcp 127.0.0.1:11434: bind: address already in use</code>，可以在此處找到解法 <a href=\"https://github.com/ollama/ollama/issues/707\">here</a> 。</p>\n<p>要重新啟動 Ollama，先停止目前的服務再重新啟動：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl stop ollama.service</span><br><span class=\"line\">ollama serve</span><br></pre></td></tr></table></figure>\n<p>為避免模型在下載後被刪除，請參考此討論 <a href=\"https://github.com/ollama/ollama/issues/1493\">here</a> 。</p>\n<h3 id=\"在-VSCode-中設定\">在 VSCode 中設定</h3>\n<ul>\n<li>安裝延伸功能後，可使用自訂模型來生成提交訊息。</li>\n<li>在介面中按下按鈕即可自動生成提交訊息。</li>\n</ul>\n<p><img src=\"https://hackmd.io/_uploads/HklK2W82C.png\" alt=\"image\"></p>\n<h1>🔁 小結</h1>\n<ul>\n<li>commitollama 是一個以隱私為重點、可替代 GitHub Copilot 的提交訊息生成工具。</li>\n<li>該工具可使用 Llama、Mistral 和 Phi3 等開源 LLM。</li>\n<li>可透過簡單的延伸功能安裝流程，輕鬆整合到 VSCode。</li>\n<li>使用者可以輕鬆取得模型、執行服務並有效生成提交訊息。</li>\n</ul>\n<h1>🔗 參考資料</h1>\n<ul>\n<li><a href=\"https://github.com/ollama/ollama/issues/707\">https://github.com/ollama/ollama/issues/707</a></li>\n<li><a href=\"https://github.com/ollama/ollama/issues/1493\">https://github.com/ollama/ollama/issues/1493</a></li>\n</ul>\n","_path":"tutorial-enhancing-commit-messages-with-commitollama-a-guide-for-vscode-and-local-llm-integration.zh-TW/","_link":"https://hsiangjenli.github.io/blog/tutorial-enhancing-commit-messages-with-commitollama-a-guide-for-vscode-and-local-llm-integration.zh-TW/","_id":"cmfj10y4s002dn42b8n4y5o67"}}