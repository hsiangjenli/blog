{"type":"getPostById","data":{"title":"[tutorial] Using Ollama with OpenCommit for Local Commit Message Generation","date":"2024-08-29T00:00:00.000Z","description":"<h2 id=\"Run-ollama\"><a href=\"#Run-ollama\" class=\"headerlink\" title=\"Run ollama\"></a>Run ollama</h2>","categories":[],"tags":[{"name":"ollama","_id":"cm6x00aji0023imqbd3ca13yo"}],"content":"<h2 id=\"Run-ollama\"><a href=\"#Run-ollama\" class=\"headerlink\" title=\"Run ollama\"></a>Run ollama</h2><span id=\"more\"></span>\n\n<h3 id=\"Start-a-container\"><a href=\"#Start-a-container\" class=\"headerlink\" title=\"Start a container\"></a>Start a container</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama-commit ollama/ollama:0.3.6</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Enter-the-Docker-container\"><a href=\"#Enter-the-Docker-container\" class=\"headerlink\" title=\"Enter the Docker container\"></a>Enter the Docker container</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker exec -it ollama-commit bash</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Ollama-CLI\"><a href=\"#Ollama-CLI\" class=\"headerlink\" title=\"Ollama CLI\"></a>Ollama CLI</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; Usage:</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;   ollama [flags]</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;   ollama [<span class=\"built_in\">command</span>]</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;</span> </span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; Available Commands:</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;   serve       Start ollama</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;   create      Create a model from a Modelfile</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;   show        Show information <span class=\"keyword\">for</span> a model</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;   run         Run a model</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;   pull        Pull a model from a registry</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;   push        Push a model to a registry</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;   list        List models</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;   ps          List running models</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;   <span class=\"built_in\">cp</span>          Copy a model</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;   <span class=\"built_in\">rm</span>          Remove a model</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;   <span class=\"built_in\">help</span>        Help about any <span class=\"built_in\">command</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;</span> </span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; Flags:</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;   -h, --<span class=\"built_in\">help</span>      <span class=\"built_in\">help</span> <span class=\"keyword\">for</span> ollama</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;   -v, --version   Show version information</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt;</span> </span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; Use <span class=\"string\">&quot;ollama [command] --help&quot;</span> <span class=\"keyword\">for</span> more information about a <span class=\"built_in\">command</span>.</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Pull-model\"><a href=\"#Pull-model\" class=\"headerlink\" title=\"Pull model\"></a>Pull model</h3><p>Click <a href=\"https://ollama.com/models\">here</a> to view the available models.</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama run gemma2:2b</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Start-a-chat\"><a href=\"#Start-a-chat\" class=\"headerlink\" title=\"Start a chat\"></a>Start a chat</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; Send a message (/? <span class=\"keyword\">for</span> <span class=\"built_in\">help</span>)</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Exit-the-chat\"><a href=\"#Exit-the-chat\" class=\"headerlink\" title=\"Exit the chat\"></a>Exit the chat</h3><p>Type <code>/bye</code> to exit the chat.</p>\n<h2 id=\"Combine-Ollama-and-opencommit-to-generate-commit-messages\"><a href=\"#Combine-Ollama-and-opencommit-to-generate-commit-messages\" class=\"headerlink\" title=\"Combine Ollama and opencommit to generate commit messages\"></a>Combine Ollama and opencommit to generate commit messages</h2><h3 id=\"Install-opencommit\"><a href=\"#Install-opencommit\" class=\"headerlink\" title=\"Install opencommit\"></a>Install opencommit</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install -g opencommit</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Generate-commit-messages-with-local-ollama-server\"><a href=\"#Generate-commit-messages-with-local-ollama-server\" class=\"headerlink\" title=\"Generate commit messages with local ollama server\"></a>Generate commit messages with local ollama server</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">OCO_AI_PROVIDER=&#x27;ollama/gemma2:2b&#x27; opencommit</span><br></pre></td></tr></table></figure>\n\n<p>output:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">┌  open-commit</span><br><span class=\"line\">│</span><br><span class=\"line\">◇  1 staged files:</span><br><span class=\"line\">  README.md</span><br><span class=\"line\">│</span><br><span class=\"line\">◇  📝 Commit message generated</span><br><span class=\"line\">│</span><br><span class=\"line\">└  Generated commit message:</span><br><span class=\"line\">——————————————————</span><br><span class=\"line\">feat(README.md): add link to Ollama website and examples of running models</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">——————————————————</span><br><span class=\"line\"></span><br><span class=\"line\">│</span><br><span class=\"line\">◇  Confirm the commit message?</span><br><span class=\"line\">│  No</span><br><span class=\"line\">│</span><br><span class=\"line\">◇  Do you want to regenerate the message ?</span><br><span class=\"line\">│  Yes</span><br><span class=\"line\">│</span><br><span class=\"line\">◇  📝 Commit message generated</span><br><span class=\"line\">│</span><br><span class=\"line\">└  Generated commit message:</span><br><span class=\"line\">——————————————————</span><br><span class=\"line\">feat(README.md): add link to Ollama website and examples of model usage in README.md </span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"Error-code-127\"><a href=\"#Error-code-127\" class=\"headerlink\" title=\"Error code 127\"></a>Error code 127</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Error: llama runner process has terminated: exit status 127</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; [GIN] 2024/08/28 - 18:43:24 | 200 |      68.455µs |       127.0.0.1 | HEAD     <span class=\"string\">&quot;/&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; [GIN] 2024/08/28 - 18:43:24 | 200 |    7.845273ms |       127.0.0.1 | POST     <span class=\"string\">&quot;/api/show&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.392Z level=INFO <span class=\"built_in\">source</span>=memory.go:309 msg=<span class=\"string\">&quot;offload to cpu&quot;</span> layers.requested=-1 layers.model=33 layers.offload=0 layers.split=<span class=\"string\">&quot;&quot;</span> memory.available=<span class=\"string\">&quot;[7.3 GiB]&quot;</span> memory.required.full=<span class=\"string\">&quot;5.5 GiB&quot;</span> memory.required.partial=<span class=\"string\">&quot;0 B&quot;</span> memory.required.kv=<span class=\"string\">&quot;1.0 GiB&quot;</span> memory.required.allocations=<span class=\"string\">&quot;[5.5 GiB]&quot;</span> memory.weights.total=<span class=\"string\">&quot;4.7 GiB&quot;</span> memory.weights.repeating=<span class=\"string\">&quot;4.6 GiB&quot;</span> memory.weights.nonrepeating=<span class=\"string\">&quot;105.0 MiB&quot;</span> memory.graph.full=<span class=\"string\">&quot;560.0 MiB&quot;</span> memory.graph.partial=<span class=\"string\">&quot;585.0 MiB&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.392Z level=INFO <span class=\"built_in\">source</span>=server.go:391 msg=<span class=\"string\">&quot;starting llama server&quot;</span> cmd=<span class=\"string\">&quot;/tmp/ollama731275887/runners/cpu/ollama_llama_server --model /root/.ollama/models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 --ctx-size 8192 --batch-size 512 --embedding --log-disable --no-mmap --parallel 4 --port 33357&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.392Z level=INFO <span class=\"built_in\">source</span>=sched.go:450 msg=<span class=\"string\">&quot;loaded runners&quot;</span> count=1</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.393Z level=INFO <span class=\"built_in\">source</span>=server.go:591 msg=<span class=\"string\">&quot;waiting for llama runner to start responding&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.393Z level=INFO <span class=\"built_in\">source</span>=server.go:625 msg=<span class=\"string\">&quot;waiting for server to become available&quot;</span> status=<span class=\"string\">&quot;llm server error&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; /tmp/ollama731275887/runners/cpu/ollama_llama_server: error <span class=\"keyword\">while</span> loading shared libraries: libllama.so: cannot open shared object file: No such file or directory</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.644Z level=ERROR <span class=\"built_in\">source</span>=sched.go:456 msg=<span class=\"string\">&quot;error loading llama server&quot;</span> error=<span class=\"string\">&quot;llama runner process has terminated: exit status 127&quot;</span></span></span><br><span class=\"line\">[GIN] 2024/08/28 - 18:43:24 | 500 |  266.021797ms |       127.0.0.1 | POST     &quot;/api/chat&quot;</span><br></pre></td></tr></table></figure>\n\n<p>The error code occurs when the Docker image version is greater than <code>0.3.6</code>. Therefore, you need to pull the ollama image with version <code>0.3.6</code> and run the container. Click <a href=\"https://github.com/ollama/ollama/issues/6541\">here</a> to view the discussion.</p>\n","_path":"tutorial_ollama_opencommit/","_link":"https://hsiangjenli.github.io/blog/tutorial_ollama_opencommit/","_id":"cm6x00aj90012imqb6myw18io"}}