{"type":"getPostById","data":{"title":"[tutorial] 使用 Ollama 與 OpenCommit 在本地生成提交訊息","date":"2024-08-28T16:00:00.000Z","description":"<blockquote>\n<p>註記：此頁為由 AI（gpt-5-mini-2025-08-07）自動翻譯自英文原文，可能含有少量不準確之處。</p>\n</blockquote>\n<h1>📌 介紹</h1>\n<p>本文說明如何使用 Ollama 與 OpenCommit 在本地生成提交訊息。內容包含在 Docker 容器中執行 Ollama 的概覽、使用 Ollama CLI 的指示，以及如何將 Ollama 與 OpenCommit 結合以生成提交訊息。</p>","categories":[],"tags":[{"name":"ollama","_id":"cmg9uwret004xqe3i8pgj4nxl"}],"content":"<blockquote>\n<p>註記：此頁為由 AI（gpt-5-mini-2025-08-07）自動翻譯自英文原文，可能含有少量不準確之處。</p>\n</blockquote>\n<h1>📌 介紹</h1>\n<p>本文說明如何使用 Ollama 與 OpenCommit 在本地生成提交訊息。內容包含在 Docker 容器中執行 Ollama 的概覽、使用 Ollama CLI 的指示，以及如何將 Ollama 與 OpenCommit 結合以生成提交訊息。</p>\n<span id=\"more\"></span>\n<h1>🚀 快速開始</h1>\n<h3 id=\"啟動容器\">啟動容器</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama-commit ollama/ollama:0.3.6</span><br></pre></td></tr></table></figure>\n<h3 id=\"進入-Docker-容器\">進入 Docker 容器</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker exec -it ollama-commit bash</span><br></pre></td></tr></table></figure>\n<h3 id=\"拉取模型\">拉取模型</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama run gemma2:2b</span><br></pre></td></tr></table></figure>\n<h3 id=\"開始聊天\">開始聊天</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; Send a message (/? <span class=\"keyword\">for</span> <span class=\"built_in\">help</span>)</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"離開聊天\">離開聊天</h3>\n<p>輸入 <code>/bye</code> 以離開聊天。</p>\n<h3 id=\"安裝-opencommit\">安裝 opencommit</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install -g opencommit</span><br></pre></td></tr></table></figure>\n<h3 id=\"使用本地-ollama-伺服器生成提交訊息\">使用本地 ollama 伺服器生成提交訊息</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">OCO_AI_PROVIDER=&#x27;ollama/gemma2:2b&#x27; opencommit</span><br></pre></td></tr></table></figure>\n<p>輸出：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">┌  open-commit</span><br><span class=\"line\">│</span><br><span class=\"line\">◇  1 staged files:</span><br><span class=\"line\">  README.md</span><br><span class=\"line\">│</span><br><span class=\"line\">◇  📝 Commit message generated</span><br><span class=\"line\">│</span><br><span class=\"line\">└  Generated commit message:</span><br><span class=\"line\">——————————————————</span><br><span class=\"line\">feat(README.md): add link to Ollama website and examples of running models</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">——————————————————</span><br><span class=\"line\"></span><br><span class=\"line\">│</span><br><span class=\"line\">◇  Confirm the commit message?</span><br><span class=\"line\">│  No</span><br><span class=\"line\">│</span><br><span class=\"line\">◇  Do you want to regenerate the message ?</span><br><span class=\"line\">│  Yes</span><br><span class=\"line\">│</span><br><span class=\"line\">◇  📝 Commit message generated</span><br><span class=\"line\">│</span><br><span class=\"line\">└  Generated commit message:</span><br><span class=\"line\">——————————————————</span><br><span class=\"line\">feat(README.md): add link to Ollama website and examples of model usage in README.md </span><br></pre></td></tr></table></figure>\n<h2 id=\"錯誤碼-127\">錯誤碼 127</h2>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Error: llama runner process has terminated: exit status 127</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; [GIN] 2024/08/28 - 18:43:24 | 200 |      68.455µs |       127.0.0.1 | HEAD     <span class=\"string\">&quot;/&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; [GIN] 2024/08/28 - 18:43:24 | 200 |    7.845273ms |       127.0.0.1 | POST     <span class=\"string\">&quot;/api/show&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.392Z level=INFO <span class=\"built_in\">source</span>=memory.go:309 msg=<span class=\"string\">&quot;offload to cpu&quot;</span> layers.requested=-1 layers.model=33 layers.offload=0 layers.split=<span class=\"string\">&quot;&quot;</span> memory.available=<span class=\"string\">&quot;[7.3 GiB]&quot;</span> memory.required.full=<span class=\"string\">&quot;5.5 GiB&quot;</span> memory.required.partial=<span class=\"string\">&quot;0 B&quot;</span> memory.required.kv=<span class=\"string\">&quot;1.0 GiB&quot;</span> memory.required.allocations=<span class=\"string\">&quot;[5.5 GiB]&quot;</span> memory.weights.total=<span class=\"string\">&quot;4.7 GiB&quot;</span> memory.weights.repeating=<span class=\"string\">&quot;4.6 GiB&quot;</span> memory.weights.nonrepeating=<span class=\"string\">&quot;105.0 MiB&quot;</span> memory.graph.full=<span class=\"string\">&quot;560.0 MiB&quot;</span> memory.graph.partial=<span class=\"string\">&quot;585.0 MiB&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.392Z level=INFO <span class=\"built_in\">source</span>=server.go:391 msg=<span class=\"string\">&quot;starting llama server&quot;</span> cmd=<span class=\"string\">&quot;/tmp/ollama731275887/runners/cpu/ollama_llama_server --model /root/.ollama/models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 --ctx-size 8192 --batch-size 512 --embedding --log-disable --no-mmap --parallel 4 --port 33357&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.392Z level=INFO <span class=\"built_in\">source</span>=sched.go:450 msg=<span class=\"string\">&quot;loaded runners&quot;</span> count=1</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.393Z level=INFO <span class=\"built_in\">source</span>=server.go:591 msg=<span class=\"string\">&quot;waiting for llama runner to start responding&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.393Z level=INFO <span class=\"built_in\">source</span>=server.go:625 msg=<span class=\"string\">&quot;waiting for server to become available&quot;</span> status=<span class=\"string\">&quot;llm server error&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; /tmp/ollama731275887/runners/cpu/ollama_llama_server: error <span class=\"keyword\">while</span> loading shared libraries: libllama.so: cannot open shared object file: No such file or directory</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.644Z level=ERROR <span class=\"built_in\">source</span>=sched.go:456 msg=<span class=\"string\">&quot;error loading llama server&quot;</span> error=<span class=\"string\">&quot;llama runner process has terminated: exit status 127&quot;</span></span></span><br><span class=\"line\">[GIN] 2024/08/28 - 18:43:24 | 500 |  266.021797ms |       127.0.0.1 | POST     &quot;/api/chat&quot;</span><br></pre></td></tr></table></figure>\n<p>當 Docker 映像版本大於 <code>0.3.6</code> 時會發生此錯誤碼。因此，您需要拉取版本為 <code>0.3.6</code> 的 ollama 映像並執行該容器。請點擊 <a href=\"https://github.com/ollama/ollama/issues/6541\">here</a> 查看討論。</p>\n<h1>🔁 重點回顧</h1>\n<ul>\n<li>Ollama 允許使用 AI 模型生成提交訊息。</li>\n<li>本文詳述在 Docker 環境中設置 Ollama 的步驟。</li>\n<li>OpenCommit 被整合以簡化使用 AI 模型生成提交訊息的流程。</li>\n<li>使用者可以透過聊天介面與 AI 模型互動。</li>\n</ul>\n<h1>🔗 參考資料</h1>\n<ul>\n<li><a href=\"https://ollama.com/models\">https://ollama.com/models</a></li>\n<li><a href=\"https://github.com/ollama/ollama/issues/6541\">https://github.com/ollama/ollama/issues/6541</a></li>\n</ul>\n","_path":"tutorial-using-ollama-with-opencommit-for-local-commit-message-generation.zh-TW/","_link":"https://hsiangjenli.github.io/blog/tutorial-using-ollama-with-opencommit-for-local-commit-message-generation.zh-TW/","_id":"cmg9uwreg002zqe3ibltbh7co"}}