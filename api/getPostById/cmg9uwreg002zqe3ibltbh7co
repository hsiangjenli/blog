{"type":"getPostById","data":{"title":"[tutorial] ä½¿ç”¨ Ollama èˆ‡ OpenCommit åœ¨æœ¬åœ°ç”Ÿæˆæäº¤è¨Šæ¯","date":"2024-08-28T16:00:00.000Z","description":"<blockquote>\n<p>è¨»è¨˜ï¼šæ­¤é ç‚ºç”± AIï¼ˆgpt-5-mini-2025-08-07ï¼‰è‡ªå‹•ç¿»è­¯è‡ªè‹±æ–‡åŸæ–‡ï¼Œå¯èƒ½å«æœ‰å°‘é‡ä¸æº–ç¢ºä¹‹è™•ã€‚</p>\n</blockquote>\n<h1>ğŸ“Œ ä»‹ç´¹</h1>\n<p>æœ¬æ–‡èªªæ˜å¦‚ä½•ä½¿ç”¨ Ollama èˆ‡ OpenCommit åœ¨æœ¬åœ°ç”Ÿæˆæäº¤è¨Šæ¯ã€‚å…§å®¹åŒ…å«åœ¨ Docker å®¹å™¨ä¸­åŸ·è¡Œ Ollama çš„æ¦‚è¦½ã€ä½¿ç”¨ Ollama CLI çš„æŒ‡ç¤ºï¼Œä»¥åŠå¦‚ä½•å°‡ Ollama èˆ‡ OpenCommit çµåˆä»¥ç”Ÿæˆæäº¤è¨Šæ¯ã€‚</p>","categories":[],"tags":[{"name":"ollama","_id":"cmg9uwret004xqe3i8pgj4nxl"}],"content":"<blockquote>\n<p>è¨»è¨˜ï¼šæ­¤é ç‚ºç”± AIï¼ˆgpt-5-mini-2025-08-07ï¼‰è‡ªå‹•ç¿»è­¯è‡ªè‹±æ–‡åŸæ–‡ï¼Œå¯èƒ½å«æœ‰å°‘é‡ä¸æº–ç¢ºä¹‹è™•ã€‚</p>\n</blockquote>\n<h1>ğŸ“Œ ä»‹ç´¹</h1>\n<p>æœ¬æ–‡èªªæ˜å¦‚ä½•ä½¿ç”¨ Ollama èˆ‡ OpenCommit åœ¨æœ¬åœ°ç”Ÿæˆæäº¤è¨Šæ¯ã€‚å…§å®¹åŒ…å«åœ¨ Docker å®¹å™¨ä¸­åŸ·è¡Œ Ollama çš„æ¦‚è¦½ã€ä½¿ç”¨ Ollama CLI çš„æŒ‡ç¤ºï¼Œä»¥åŠå¦‚ä½•å°‡ Ollama èˆ‡ OpenCommit çµåˆä»¥ç”Ÿæˆæäº¤è¨Šæ¯ã€‚</p>\n<span id=\"more\"></span>\n<h1>ğŸš€ å¿«é€Ÿé–‹å§‹</h1>\n<h3 id=\"å•Ÿå‹•å®¹å™¨\">å•Ÿå‹•å®¹å™¨</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama-commit ollama/ollama:0.3.6</span><br></pre></td></tr></table></figure>\n<h3 id=\"é€²å…¥-Docker-å®¹å™¨\">é€²å…¥ Docker å®¹å™¨</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker exec -it ollama-commit bash</span><br></pre></td></tr></table></figure>\n<h3 id=\"æ‹‰å–æ¨¡å‹\">æ‹‰å–æ¨¡å‹</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama run gemma2:2b</span><br></pre></td></tr></table></figure>\n<h3 id=\"é–‹å§‹èŠå¤©\">é–‹å§‹èŠå¤©</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; Send a message (/? <span class=\"keyword\">for</span> <span class=\"built_in\">help</span>)</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"é›¢é–‹èŠå¤©\">é›¢é–‹èŠå¤©</h3>\n<p>è¼¸å…¥ <code>/bye</code> ä»¥é›¢é–‹èŠå¤©ã€‚</p>\n<h3 id=\"å®‰è£-opencommit\">å®‰è£ opencommit</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install -g opencommit</span><br></pre></td></tr></table></figure>\n<h3 id=\"ä½¿ç”¨æœ¬åœ°-ollama-ä¼ºæœå™¨ç”Ÿæˆæäº¤è¨Šæ¯\">ä½¿ç”¨æœ¬åœ° ollama ä¼ºæœå™¨ç”Ÿæˆæäº¤è¨Šæ¯</h3>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">OCO_AI_PROVIDER=&#x27;ollama/gemma2:2b&#x27; opencommit</span><br></pre></td></tr></table></figure>\n<p>è¼¸å‡ºï¼š</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">â”Œ  open-commit</span><br><span class=\"line\">â”‚</span><br><span class=\"line\">â—‡  1 staged files:</span><br><span class=\"line\">  README.md</span><br><span class=\"line\">â”‚</span><br><span class=\"line\">â—‡  ğŸ“ Commit message generated</span><br><span class=\"line\">â”‚</span><br><span class=\"line\">â””  Generated commit message:</span><br><span class=\"line\">â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”</span><br><span class=\"line\">feat(README.md): add link to Ollama website and examples of running models</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”</span><br><span class=\"line\"></span><br><span class=\"line\">â”‚</span><br><span class=\"line\">â—‡  Confirm the commit message?</span><br><span class=\"line\">â”‚  No</span><br><span class=\"line\">â”‚</span><br><span class=\"line\">â—‡  Do you want to regenerate the message ?</span><br><span class=\"line\">â”‚  Yes</span><br><span class=\"line\">â”‚</span><br><span class=\"line\">â—‡  ğŸ“ Commit message generated</span><br><span class=\"line\">â”‚</span><br><span class=\"line\">â””  Generated commit message:</span><br><span class=\"line\">â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”</span><br><span class=\"line\">feat(README.md): add link to Ollama website and examples of model usage in README.md </span><br></pre></td></tr></table></figure>\n<h2 id=\"éŒ¯èª¤ç¢¼-127\">éŒ¯èª¤ç¢¼ 127</h2>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Error: llama runner process has terminated: exit status 127</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; [GIN] 2024/08/28 - 18:43:24 | 200 |      68.455Âµs |       127.0.0.1 | HEAD     <span class=\"string\">&quot;/&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; [GIN] 2024/08/28 - 18:43:24 | 200 |    7.845273ms |       127.0.0.1 | POST     <span class=\"string\">&quot;/api/show&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.392Z level=INFO <span class=\"built_in\">source</span>=memory.go:309 msg=<span class=\"string\">&quot;offload to cpu&quot;</span> layers.requested=-1 layers.model=33 layers.offload=0 layers.split=<span class=\"string\">&quot;&quot;</span> memory.available=<span class=\"string\">&quot;[7.3 GiB]&quot;</span> memory.required.full=<span class=\"string\">&quot;5.5 GiB&quot;</span> memory.required.partial=<span class=\"string\">&quot;0 B&quot;</span> memory.required.kv=<span class=\"string\">&quot;1.0 GiB&quot;</span> memory.required.allocations=<span class=\"string\">&quot;[5.5 GiB]&quot;</span> memory.weights.total=<span class=\"string\">&quot;4.7 GiB&quot;</span> memory.weights.repeating=<span class=\"string\">&quot;4.6 GiB&quot;</span> memory.weights.nonrepeating=<span class=\"string\">&quot;105.0 MiB&quot;</span> memory.graph.full=<span class=\"string\">&quot;560.0 MiB&quot;</span> memory.graph.partial=<span class=\"string\">&quot;585.0 MiB&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.392Z level=INFO <span class=\"built_in\">source</span>=server.go:391 msg=<span class=\"string\">&quot;starting llama server&quot;</span> cmd=<span class=\"string\">&quot;/tmp/ollama731275887/runners/cpu/ollama_llama_server --model /root/.ollama/models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 --ctx-size 8192 --batch-size 512 --embedding --log-disable --no-mmap --parallel 4 --port 33357&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.392Z level=INFO <span class=\"built_in\">source</span>=sched.go:450 msg=<span class=\"string\">&quot;loaded runners&quot;</span> count=1</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.393Z level=INFO <span class=\"built_in\">source</span>=server.go:591 msg=<span class=\"string\">&quot;waiting for llama runner to start responding&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.393Z level=INFO <span class=\"built_in\">source</span>=server.go:625 msg=<span class=\"string\">&quot;waiting for server to become available&quot;</span> status=<span class=\"string\">&quot;llm server error&quot;</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; /tmp/ollama731275887/runners/cpu/ollama_llama_server: error <span class=\"keyword\">while</span> loading shared libraries: libllama.so: cannot open shared object file: No such file or directory</span></span><br><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">&gt;&gt; time=2024-08-28T18:43:24.644Z level=ERROR <span class=\"built_in\">source</span>=sched.go:456 msg=<span class=\"string\">&quot;error loading llama server&quot;</span> error=<span class=\"string\">&quot;llama runner process has terminated: exit status 127&quot;</span></span></span><br><span class=\"line\">[GIN] 2024/08/28 - 18:43:24 | 500 |  266.021797ms |       127.0.0.1 | POST     &quot;/api/chat&quot;</span><br></pre></td></tr></table></figure>\n<p>ç•¶ Docker æ˜ åƒç‰ˆæœ¬å¤§æ–¼ <code>0.3.6</code> æ™‚æœƒç™¼ç”Ÿæ­¤éŒ¯èª¤ç¢¼ã€‚å› æ­¤ï¼Œæ‚¨éœ€è¦æ‹‰å–ç‰ˆæœ¬ç‚º <code>0.3.6</code> çš„ ollama æ˜ åƒä¸¦åŸ·è¡Œè©²å®¹å™¨ã€‚è«‹é»æ“Š <a href=\"https://github.com/ollama/ollama/issues/6541\">here</a> æŸ¥çœ‹è¨è«–ã€‚</p>\n<h1>ğŸ” é‡é»å›é¡§</h1>\n<ul>\n<li>Ollama å…è¨±ä½¿ç”¨ AI æ¨¡å‹ç”Ÿæˆæäº¤è¨Šæ¯ã€‚</li>\n<li>æœ¬æ–‡è©³è¿°åœ¨ Docker ç’°å¢ƒä¸­è¨­ç½® Ollama çš„æ­¥é©Ÿã€‚</li>\n<li>OpenCommit è¢«æ•´åˆä»¥ç°¡åŒ–ä½¿ç”¨ AI æ¨¡å‹ç”Ÿæˆæäº¤è¨Šæ¯çš„æµç¨‹ã€‚</li>\n<li>ä½¿ç”¨è€…å¯ä»¥é€éèŠå¤©ä»‹é¢èˆ‡ AI æ¨¡å‹äº’å‹•ã€‚</li>\n</ul>\n<h1>ğŸ”— åƒè€ƒè³‡æ–™</h1>\n<ul>\n<li><a href=\"https://ollama.com/models\">https://ollama.com/models</a></li>\n<li><a href=\"https://github.com/ollama/ollama/issues/6541\">https://github.com/ollama/ollama/issues/6541</a></li>\n</ul>\n","_path":"tutorial-using-ollama-with-opencommit-for-local-commit-message-generation.zh-TW/","_link":"https://hsiangjenli.github.io/blog/tutorial-using-ollama-with-opencommit-for-local-commit-message-generation.zh-TW/","_id":"cmg9uwreg002zqe3ibltbh7co"}}