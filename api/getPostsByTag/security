{"type":"getPostsByTag","data":[{"title":"[note] Introduction to Antropic's automated AI safety auditing framework `safety-research/petri`","date":"2025-11-03T16:00:00.000Z","description":"<blockquote>\n<p>Note: This page is an AI-generated (gpt-5-mini-2025-08-07) translation from Traditional Chinese and may contain minor inaccuracies.</p>\n</blockquote>\n<h1>ğŸ“Œ Introduction</h1>\n<p>Petri is a red-team tool for AI safety testing that simulates realistic interaction scenarios to detect potential risks in models. Through collaboration among an Auditor, a Target model, and a Judge model, it performs various tasks such as general audits, multi-model comparisons, and whistleblowing tests, checking whether models leak information or exhibit bias, improving AI safety and reliability in complex situations</p>","categories":[],"tags":[{"name":"llm","_id":"cmhkkn4pr000qmx233p5t5vu5"},{"name":"security","_id":"cmhkkn4ps000smx23e2ki1j9s"}],"_path":"note-antropic-ai-safety-research-petri.en/","_link":"https://hsiangjenli.github.io/blog/note-antropic-ai-safety-research-petri.en/","_id":"cmhkkn4pk0006mx234jmmesd6"},{"title":"[note] ä»‹ç´¹ Antropic çš„è‡ªå‹•åŒ– AI å®‰å…¨å¯©è¨ˆæ¡†æ¶ `safety-research/petri`","date":"2025-11-03T16:00:00.000Z","description":"<h1>ğŸ“Œ ç°¡ä»‹</h1>\n<p>Petri æ˜¯ä¸€å€‹æ–¼ AI å®‰å…¨æ¸¬è©¦çš„ç´…éšŠå·¥å…·ï¼Œæ¨¡æ“¬çœŸå¯¦äº’å‹•æƒ…å¢ƒï¼Œæª¢æ¸¬æ¨¡å‹æ½›åœ¨é¢¨éšªã€‚é€éå¯©è¨ˆæ¨¡å‹ï¼ˆAuditorï¼‰ã€ç›®æ¨™æ¨¡å‹ï¼ˆTargetï¼‰èˆ‡è£åˆ¤æ¨¡å‹ï¼ˆJudgeï¼‰çš„å”ä½œï¼ŒåŸ·è¡Œå¤šç¨®ä»»å‹™ï¼Œä¾‹å¦‚é€šç”¨å¯©è¨ˆã€å¤šæ¨¡å‹æ¯”è¼ƒåŠå¹å“¨è€…æ¸¬è©¦ï¼Œæ¸¬è©¦æ¨¡å‹æ˜¯å¦æœ‰è³‡è¨Šæ´©æ¼æˆ–åè¦‹ç­‰å•é¡Œï¼Œæå‡ AI åœ¨è¤‡é›œå ´æ™¯ä¸­çš„å®‰å…¨æ€§èˆ‡å¯é æ€§</p>","categories":[],"tags":[{"name":"llm","_id":"cmhkkn4pr000qmx233p5t5vu5"},{"name":"security","_id":"cmhkkn4ps000smx23e2ki1j9s"}],"_path":"note_petri/","_link":"https://hsiangjenli.github.io/blog/note_petri/","_id":"cmhkkn4qx0031mx23a4d16yet"}]}