---
title: '[æ•™å­¸] ä½¿ç”¨ Ollama èˆ‡ OpenCommit é€²è¡Œæœ¬åœ°æäº¤è¨Šæ¯ç”Ÿæˆ'
date: '2024-08-29'
lang: zh-TW
updated: 2025-02-28 (Refactored by ChatGPT-4o Mini)
author:
- Hsiang-Jen Li
- ' & ChatGPT-4o Mini'
tags:
- ollama
toc: true
translation_key: tutorial-using-ollama-with-opencommit-for-local-commit-message-generation
slug: tutorial-using-ollama-with-opencommit-for-local-commit-message-generation
source_sha: 7b5ad48eaf49f87b2f8799479740c20362eb6281e183bb6b64b1822bf79b5008
origin_lang: en
---

# ğŸ“Œ ä»‹ç´¹
æœ¬æ–‡èªªæ˜å¦‚ä½•å°‡ Ollama èˆ‡ OpenCommit çµåˆï¼Œç”¨æ–¼åœ¨æœ¬åœ°ç”Ÿæˆæäº¤è¨Šæ¯ã€‚å…§å®¹åŒ…æ‹¬åœ¨ Docker å®¹å™¨ä¸­é‹è¡Œ Ollama çš„æ¦‚è¿°ã€ä½¿ç”¨ Ollama CLI çš„æŒ‡å¼•ï¼Œä»¥åŠå¦‚ä½•å°‡ Ollama èˆ‡ OpenCommit çµåˆä»¥ç”Ÿæˆæäº¤è¨Šæ¯ã€‚
<!-- more -->

# ğŸš€ å¿«é€Ÿé–‹å§‹
### å•Ÿå‹•å®¹å™¨
```shell
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama-commit ollama/ollama:0.3.6
```

### é€²å…¥ Docker å®¹å™¨
```shell
docker exec -it ollama-commit bash
```

### æ‹‰å–æ¨¡å‹
```shell
ollama run gemma2:2b
```

### é–‹å§‹å°è©±
```shell
>>> Send a message (/? for help)
```

### é€€å‡ºå°è©±
è¼¸å…¥ `/bye` é€€å‡ºå°è©±ã€‚

### å®‰è£ opencommit
```shell
npm install -g opencommit
```

### ä½¿ç”¨æœ¬åœ° Ollama ä¼ºæœå™¨ç”Ÿæˆæäº¤è¨Šæ¯
```shell
OCO_AI_PROVIDER='ollama/gemma2:2b' opencommit
```

output:

```shell
â”Œ  open-commit
â”‚
â—‡  1 staged files:
  README.md
â”‚
â—‡  ğŸ“ Commit message generated
â”‚
â””  Generated commit message:
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
feat(README.md): add link to Ollama website and examples of running models


â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

â”‚
â—‡  Confirm the commit message?
â”‚  No
â”‚
â—‡  Do you want to regenerate the message ?
â”‚  Yes
â”‚
â—‡  ğŸ“ Commit message generated
â”‚
â””  Generated commit message:
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
feat(README.md): add link to Ollama website and examples of model usage in README.md 
```


## éŒ¯èª¤ä»£ç¢¼ 127

```shell
Error: llama runner process has terminated: exit status 127

>>> [GIN] 2024/08/28 - 18:43:24 | 200 |      68.455Âµs |       127.0.0.1 | HEAD     "/"
>>> [GIN] 2024/08/28 - 18:43:24 | 200 |    7.845273ms |       127.0.0.1 | POST     "/api/show"
>>> time=2024-08-28T18:43:24.392Z level=INFO source=memory.go:309 msg="offload to cpu" layers.requested=-1 layers.model=33 layers.offload=0 layers.split="" memory.available="[7.3 GiB]" memory.required.full="5.5 GiB" memory.required.partial="0 B" memory.required.kv="1.0 GiB" memory.required.allocations="[5.5 GiB]" memory.weights.total="4.7 GiB" memory.weights.repeating="4.6 GiB" memory.weights.nonrepeating="105.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="585.0 MiB"
>>> time=2024-08-28T18:43:24.392Z level=INFO source=server.go:391 msg="starting llama server" cmd="/tmp/ollama731275887/runners/cpu/ollama_llama_server --model /root/.ollama/models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 --ctx-size 8192 --batch-size 512 --embedding --log-disable --no-mmap --parallel 4 --port 33357"
>>> time=2024-08-28T18:43:24.392Z level=INFO source=sched.go:450 msg="loaded runners" count=1
>>> time=2024-08-28T18:43:24.393Z level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
>>> time=2024-08-28T18:43:24.393Z level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server error"
>>> /tmp/ollama731275887/runners/cpu/ollama_llama_server: error while loading shared libraries: libllama.so: cannot open shared object file: No such file or directory
>>> time=2024-08-28T18:43:24.644Z level=ERROR source=sched.go:456 msg="error loading llama server" error="llama runner process has terminated: exit status 127"
[GIN] 2024/08/28 - 18:43:24 | 500 |  266.021797ms |       127.0.0.1 | POST     "/api/chat"
```

ç•¶ Docker æ˜ åƒç‰ˆæœ¬å¤§æ–¼ `0.3.6` æ™‚æœƒç™¼ç”Ÿæ­¤éŒ¯èª¤ä»£ç¢¼ã€‚å› æ­¤ï¼Œæ‚¨éœ€è¦æ‹‰å–ç‰ˆæœ¬ç‚º `0.3.6` çš„ ollama æ˜ åƒä¸¦åŸ·è¡Œè©²å®¹å™¨ã€‚é»æ“Š [æ­¤è™•](https://github.com/ollama/ollama/issues/6541) æŸ¥çœ‹è¨è«–ã€‚


# ğŸ” é‡é»å›é¡§
- Ollama å…è¨±ä½¿ç”¨ AI æ¨¡å‹ç”Ÿæˆæäº¤è¨Šæ¯ã€‚
- æœ¬æ–‡è©³ç´°èªªæ˜å¦‚ä½•åœ¨ Docker ç’°å¢ƒä¸­è¨­å®š Ollamaã€‚
- å·²æ•´åˆ OpenCommitï¼Œä»¥ç°¡åŒ–ä½¿ç”¨ AI æ¨¡å‹ç”Ÿæˆæäº¤è¨Šæ¯çš„æµç¨‹ã€‚
- ä½¿ç”¨è€…å¯ä»¥é€éèŠå¤©ä»‹é¢èˆ‡ AI æ¨¡å‹äº’å‹•ã€‚

# ğŸ”— åƒè€ƒè³‡æ–™
- https://ollama.com/models
- https://github.com/ollama/ollama/issues/6541
